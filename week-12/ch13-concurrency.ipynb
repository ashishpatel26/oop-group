{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Chapter 13: Concurrency ðŸš€\n",
    "\n",
    "- Threads\n",
    "- Multiprocessing\n",
    "- Futures \n",
    "- AsyncIO\n",
    "\n",
    "**By Will Norris**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Sequential Programming: \n",
    "- Sequential programming model is intuitive and natural\n",
    "    - Do things **one step at a time** (The way humans think)\n",
    "- In programming languages: \n",
    "    - Each of these real-world actions is an abstraction for a sequence of finer-grained actions\n",
    "    - Flow: ```Open the cupboard, select a tea, check water level in kettle, if low: add more, boil water, pour water in cup, wait for tea...``` \n",
    "- **But**, what we do while the water is boiling is up to us\n",
    "    - Do we simply wait? \n",
    "    - Or, do we do other tasks such as starting our toast or fetching the newspaper (asynchronous tasks)\n",
    "        - The whole time aware that we are waiting for our water to boil!\n",
    "- Tea kettle makers know people tend to operate asynchronously, so they add a warning for when your tea is done, to remind you to come back to the task at hand. \n",
    "    - Finding the right balance of sequentiality and asynchrony is a characteristic of efficient people, **the same is true of efficient programs**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Why Concurrency? \n",
    "- At some point it is not cost efficient to buy a faster machine (**scaling vertically**)\n",
    "- Instead of scaling computation up, we can go out (**scaling horizontally**)\n",
    "    - Allows us to use cheap hardware, and accomplish pieces of computation across a set of threads/processors/nodes \n",
    "- In modern computing, we can divide the problem entirely across nodes (processors) \n",
    "    - In legacy computing, we could take advantage of \"switching\", which means rapidly swapping between threads on a single process to accompish multiple things \"at once\" (time sharing systems) \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Multiple Processes: \n",
    "__Motivating Factors:__\n",
    "- **Resource Utilizaton:** \n",
    "    - Programs are always waiting for external operations (File I/O), and can't do anything while they wait. Let's use that time!\n",
    "- **Fairness:**\n",
    "    - Multiple users and programs may have equal claim on the machine's resources. We want to let them share \"slices\" of time rather than give one before the other \n",
    "- **Convenience:**\n",
    "    - It is easier to write several programs that each perform a single task and have them coordinate with each other when needed than to write one big program. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Threads: \n",
    "- Threads allow multiple streams of program control flow to coexist within a process. \n",
    "- They share process-wide resources (memory, file handles) \n",
    "    - But, each thread has its own program counter, stack, and local variables \n",
    "- Threads provide a natural decomposition for exploiting hardware parallelism when we have multiple processors\n",
    "    -  multiple threads within the same program can be scheduled simultaneously on multi CPU's\n",
    "- Most modern OS's treat threads as **lightweight processses** and use them (not processes) as the basic unit of scheduling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![](https://imgur.com/5mte34P.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from threading import Thread\n",
    "\n",
    "class InputReader(Thread):\n",
    "    def run(self):\n",
    "        self.line_of_text = input()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "print(\"Enter any text and press enter: \")\n",
    "thread = InputReader()\n",
    "thread.start()\n",
    "\n",
    "count = result = 1\n",
    "while thread.is_alive():\n",
    "    result = count * count \n",
    "    count += 1\n",
    "\n",
    "print(\"calculated squares up to {0} * {0} = {1}\".format(\n",
    "    count, result))\n",
    "print(\"while you typed '{}'\".format(thread.line_of_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import json \n",
    "from urllib.request import urlopen \n",
    "import time \n",
    "import requests\n",
    "import pyowm\n",
    "\n",
    "CITIES = ['Edmonton', 'Victoria', 'Winnipeg', 'Fredericton',\n",
    "          'Halifax', 'Toronto', 'Charlottetown',\n",
    "          'Quebec', 'Regina']\n",
    "\n",
    "class TempGetter(Thread):\n",
    "    def __init__(self, city):\n",
    "        super().__init__()\n",
    "        self.city = city\n",
    "        self.owm = pyowm.OWM('be06b12aa45e1ca05a8f972f81376c6d')\n",
    "    def run(self):\n",
    "        city = self.owm.weather_at_place(self.city)\n",
    "        weather = city.get_weather()\n",
    "        self.temperature = weather.get_temperature('fahrenheit')['temp']\n",
    "        \n",
    "threads = [TempGetter(c) for c in CITIES]\n",
    "start = time.time()\n",
    "for thread in threads:\n",
    "    thread.start()\n",
    "for thread in threads:\n",
    "    thread.join()\n",
    "for thread in threads:\n",
    "    print(\"it is {0.temperature:.0f}Â°C in {0.city}\".format(thread))\n",
    "print(\n",
    "   \"Got {} temps in {} seconds\".format(\n",
    "   len(threads), time.time() - start))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### What's actually happening here? \n",
    "- 10 threads are started\n",
    "    - Remember to call ```super``` to ensure we instantiate an actual ```Thread``` object. \n",
    "    - We construct 10 thread objects from within the main thread, then run them later. \n",
    "        - Data constructed in one thread is accessible from other running threads\n",
    "- Each thread is joined with eachother \n",
    "    - Joining threads tells each one to \"wait for the thread to complete before doing anything\" \n",
    "    - This means the second for loop won't end until all 10 threads have finished \n",
    "- **In threads, all state is shared by default**\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Running on one thread instead, much slower:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "threads = [TempGetter(c) for c in CITIES]\n",
    "start = time.time()\n",
    "for thread in threads:\n",
    "    thread.run()\n",
    "for thread in threads:\n",
    "    print(\"it is {0.temperature:.0f}Â°C in {0.city}\".format(thread))\n",
    "print(\n",
    "   \"Got {} temps in {} seconds\".format(\n",
    "   len(threads), time.time() - start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Threads sound great! What's the catch?? \n",
    "- Python programmers avoid threading for several reasons: \n",
    "    - Better alternative methods to concurrent programming in Python\n",
    "    - Shared Memory\n",
    "    - Global Interpreter Lock\n",
    "    - Thread Overhead"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Shared Memory:**\n",
    "- Shared memory is both a major advantage and disadvantage of threading\n",
    "    - It is convenient to have access to all variables in memory from any thread \n",
    "    - However, this can cause horrible inconsistencies in the program state \n",
    "        - It is easy to allow one thread to change a value that another thread expected, which can cause unknown errors. \n",
    "- We can \"synchronize\" thread's access to variables, however this can get complex and improper synchronization can be hard to find."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**The Global Interpreter Lock:**\n",
    "- To efficiently manage memory , garbage collection, and calls to machine code in libraries, Python uses the Global Interpreter Lock (GIL)\n",
    "- It is impossible to turn off and it makes it impossible to properly use threads for parralell processing in python\n",
    "    - The GIL will prevent any two thread's from doing work at the exact same time, even if they have work to do. (\"doing work\" == using CPU) \n",
    "    - The GIL is released as soon as the thread starts to wait for anything \n",
    "    \n",
    "**Why do we still have the GIL?**\n",
    "- It makes the reference implementation much easier to maintain (language structure)\n",
    "- It makes single core python faster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Thread Overhead:**\n",
    "- Each thread takes up some memory (both in python and the OS kernel) to keep track of the thread state \n",
    "- Switching (jumping between threads) uses some CPU time\n",
    "    - This can be improved with proper thread management and the use of ```ThreadPool``` to help reuse threads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The better tool: Multiprocessing\n",
    "- Multiprocessing is designed for when CPU-intensive jobs can be run in parallel and mulitple cores are available. \n",
    "    - There are almost always multiple cores available now! (4-core Rasberry pi = $30)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "from multiprocessing import Process, cpu_count\n",
    "import time \n",
    "import os \n",
    "\n",
    "class MuchCPU(Process):\n",
    "    def run(self):\n",
    "        print(os.getpid())\n",
    "        for i in range(200000000):\n",
    "            pass\n",
    "        \n",
    "procs = [MuchCPU() for f in range(cpu_count())]\n",
    "t = time.time()\n",
    "for p in procs:\n",
    "    p.start()\n",
    "for p in procs: \n",
    "    p.join()\n",
    "print(\"work took {} seconds\".format(time.time()-t))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- I have a 2-core (4 virtual cores) machine, so python will spin up 4 different processes each with a unique pid (process ID) \n",
    "- This took me 8.28 seconds versus 12.96 seconds on a 2014 similar setup. \n",
    "    - This is a 36% increase in performance without increasing core count\n",
    "    \n",
    "- When I run this process on my laptop, we can see that all 4 cores hit 100% usage for a brief moment.\n",
    "    - This is because each core is simultaneously crunching 2,000,000 numbers\n",
    "    \n",
    "![](https://imgur.com/zzR5qxm.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "class MuchCPU(Thread):\n",
    "    def run(self):\n",
    "        print(os.getpid())\n",
    "        for i in range(200000000):\n",
    "            pass\n",
    "        \n",
    "procs = [MuchCPU() for f in range(cpu_count())]\n",
    "t = time.time()\n",
    "for p in procs:\n",
    "    p.start()\n",
    "for p in procs: \n",
    "    p.join()\n",
    "print(\"work took {} seconds\".format(time.time()-t))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Here we subclass on ```Thread``` instead of ```Process```\n",
    "    - This means that each thread is sharing the same process \n",
    "- **Why is it not 4x slower?**\n",
    "    - When we use all 4 processes on my laptop, they have to share computation with the other stuff happening on my laptop. \n",
    "    - When we use a single process and multithread, the remaining three processes can do laptop stuff "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Multiprocessing Pools\n",
    "- It can get really hard to allocate processes for each task. \n",
    "- We need something to manage where/when code is running and when each process interacts with eachother.\n",
    "- Pools are designed to distribute tasks to available processors! \n",
    "    - Use FIFO scheduling \n",
    "    - Map reduce architecture: \n",
    "        - maps input to different processors and collects the output from all the processors\n",
    "        - After execution, returns output in form of a list or array\n",
    "        - Nothing is returned until all processes finish"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### When Should I use Pool or Process???\n",
    "**Pool**\n",
    "- Pool allows you to do multiple jobs per process, which can make your program easier to parallelize\n",
    "- Much better for large jobs\n",
    "- Example: You have a million tasks to execute in parallel\n",
    "    - You can create a ```Pool``` with ```num_processes = CPU_count``` and pass them to ```pool.map```\n",
    "    - The pool will distribute those tasks to the worker processes and collects the return values in a list for the parent process\n",
    "    \n",
    "**Process**\n",
    "- If you have a small subset of tasks then a Pool may be overkill for the task \n",
    "    - All the pickling and moving and scheduling slows things down!\n",
    "- File IO\n",
    "    - In pooling if there is a long IO operation, due to the FIFO scheduling, the core will wait to schedule another process\n",
    "    - Process would suspend the IO process and schedule another one "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import random \n",
    "import time\n",
    "import multiprocessing\n",
    "\n",
    "data = (\n",
    "    ['a', '2'], ['b', '4'], ['c', '6'], ['d', '8'],\n",
    "    ['e', '1'], ['f', '3'], ['g', '5'], ['h', '7']\n",
    ")\n",
    "\n",
    "def mp_worker(input1):\n",
    "    inputs = input1[0]\n",
    "    the_time = input1[1]\n",
    "    print(\" Processs {}\\tWaiting {} seconds\".format(inputs, the_time))\n",
    "    time.sleep(int(the_time))\n",
    "    print(\" Process {}\\tDONE\".format(inputs))\n",
    "\n",
    "def mp_handler():\n",
    "    p = multiprocessing.Pool(2)\n",
    "    p.map(mp_worker, data)\n",
    "\n",
    "mp_handler()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### So, what is happening here? \n",
    "- We have two pools to corrall our processes into. This means that each pool will automatically take the next task in our list of tasks as they finish. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Queues: \n",
    "- Queues are good for sending messages from one process into one or more other processes\n",
    "    - Any picklable object can be sent into a Queue\n",
    "        - Pickling is expensive, especially on large objects!!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Queues Example:**\n",
    "- We have a very simple reader and writer of messages off of a queue \n",
    "- The writer sends a lot of integers to the reader \n",
    "    - when the writer runs out of numbers, it sends a ```\"DONE\"``` message\n",
    "    - when the reader receives a ```\"DONE\"``` message, it breaks the read loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "from multiprocessing import Process, Queue\n",
    "import time\n",
    "import sys \n",
    "\n",
    "def reader_proc(queue):\n",
    "    # Read from the queue. spawned as seperate process \n",
    "    while True:\n",
    "        msg = queue.get() # just read from queue \n",
    "        if (msg == \"DONE\"):\n",
    "            break\n",
    "            \n",
    "def writer(count, queue):\n",
    "    # Write to our queue\n",
    "    for ii in range(0, count):\n",
    "        queue.put(ii)\n",
    "    queue.put('DONE')\n",
    "        \n",
    "pqueue = Queue()\n",
    "for count in [10**4, 10**5, 10**6]:\n",
    "    # reade_proc() reads pqueue as sperate process\n",
    "    reader_p = Process(target=reader_proc, args=((pqueue),))\n",
    "    reader_p.daemon = True\n",
    "    reader_p.start()\n",
    "    \n",
    "    _start = time.time()\n",
    "    writer(count, pqueue)\n",
    "    reader_p.join()\n",
    "    print(\"Sending {0} numbers to Queue() took {1} seconds\".format(count,(time.time()-_start)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## So... What's the Catch with Multiprocessing? \n",
    "\n",
    "- **\"There is no best way to do concurrency; this is especially true in Python\"**\n",
    "    - Every problem is approached differently in parallel, part of the challenge is choosing the best way to parallelize. \n",
    "\n",
    "- Primary drawback with multiprocessing is that sharing dta between processes is super expensive. \n",
    "    - We have to employ some data structure that pickles data (queue, pipe, etc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- Thus, due to the overhead of communication, **multicprocessing is best applied when:**\n",
    "    - Information passed between processes is small \n",
    "    - The work done in each individual process is very large\n",
    "\n",
    "Additional Drawbacks:\n",
    "- It can also be very difficult to tell which process is accessing each variable in memory and when\n",
    "    - It is easily possible for one process to overwrite a variable in memory when another process expects it to remain unchanged "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Asynchronous Concurrency: Futures\n",
    "- Futures \"Provide a high-level interface for asynchronously executing callables\" - Python 3.x docs\n",
    "- They allow us to structure our code such that it is easier to track down when we alter the shared state\n",
    "- A future is just an object that wraps a function call. \n",
    "    - The function is run in the background in a thread or process \n",
    "    - Checks if things have completed and get results when they do \n",
    "    \n",
    "- Fun Fact: In Computer Science, future refers to a construct that can be used for synchronization when uszing concurrent programming techniques\n",
    "    - The future is a way to describe the result of a process or thread before it has finished processing (pending result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from concurrent.futures import as_completed\n",
    "import os \n",
    "import urllib.request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "def downloader(url):\n",
    "    req = urllib.request.urlopen(url)\n",
    "    filename = os.path.basename(url)\n",
    "    ext = os.path.splitext(url)[1]\n",
    "    if not ext: \n",
    "        raise RuntimeError(\"URL does not contain an extension\")\n",
    "    \n",
    "    with open(filename, 'wb') as file_handle:\n",
    "        while True:\n",
    "            chunk = req.read(1024)\n",
    "            if not chunk:\n",
    "                break\n",
    "            file_handle.write(chunk)\n",
    "    msg = 'Finished downloading {filename}'.format(filename=filename)\n",
    "    return msg\n",
    "\n",
    "def main(urls):\n",
    "    with ThreadPoolExecutor(max_workers=5) as executor:\n",
    "        futures = [executor.submit(downloader, url) for url in urls]\n",
    "        for future in as_completed(futures):\n",
    "            print(future.result())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "urls = [\"http://www.irs.gov/pub/irs-pdf/f1040.pdf\",\n",
    "            \"http://www.irs.gov/pub/irs-pdf/f1040a.pdf\",\n",
    "            \"http://www.irs.gov/pub/irs-pdf/f1040ez.pdf\",\n",
    "            \"http://www.irs.gov/pub/irs-pdf/f1040es.pdf\",\n",
    "            \"http://www.irs.gov/pub/irs-pdf/f1040sb.pdf\"]\n",
    "main(urls)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### So, what's actually going on here? \n",
    "- In main we instantiate our thread pool: \n",
    "    - Our pool has 5 workers\n",
    "    - We use list comprehension to create a group of futures (jobs) and then we call the ```as_complete()```.\n",
    "        - This function is an iterator that yields the futures as they complete. \n",
    "        - When they complete, we print the result, which is the string returned from ```downloader()```\n",
    "\n",
    "**Futures' Structure:** \n",
    "- Once the ```executor``` has been constructed, we can ```submit``` jobs to it\n",
    "    - The ```submit()``` method immediately returns a ```Future``` object, which is a promise to give a result eventually\n",
    "- Futures have built in ```queue``` functionality: \n",
    "    - We can construct a ```queue``` of futures (essentially all running) and loop through the queue over and over\n",
    "        - If the future at the end of the queue is still running, put it back at the front of the queue\n",
    "        - If the future is still running, then get the ```result()```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## AsyncIO: \n",
    "**Current state of the art in Python concurrent programming**\n",
    "\n",
    "- Combines futures, coroutines, and an Event Loop\n",
    "- Can be used for a few different concurrent tasks, but designed for network I/O\n",
    "    - Most network apps spend a lot of time waiting for data to come in\n",
    "    - We handle each client in a seperate thread, but threads use memory and other resources, so AsyncIO uses coroutines\n",
    "\n",
    "**Example:** You have a single core machine. You receive a request and need to make two database queries. Each query takes 50ms. \n",
    "- Synchronously: You must completely resolve the first query before starting the second. Total time = 100ms\n",
    "- Asynchronous: You can send off query one, send off query two, then wait for each one individually. Total time = 50ms\n",
    "    \n",
    "**From Hackernoon.com:**\n",
    "\n",
    "    \"Asyncio is a beautiful symphony between an Event loop, Tasks and Coroutines all coming together so perfectlyâ€Šâ€”â€Šits going to make you cry.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Event Loop\n",
    "- In its most simple form, and Event Loop is just a loop that runs tasks one at a time.\n",
    "- The special part is that when the running task makes a blocking call (network request)\n",
    "    - The event loop can let another task take a turn, and remembers roughly how long the blocking task will be waiting \n",
    "    \n",
    "    *\"The event loop time is precious. If you are not making progress, you should step off the loop, so that someone else can. Event loop is the measure of progress\" -- Miklos Philips*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### The Coroutine & Task\n",
    "- Coroutines are essentially *stateful* functions\n",
    "    - When acoroutine is waiting for something it can give up control of the event loop, but save it's state for when it is ready again\n",
    "- To pause a couroutine we use ```await other_coroutine```\n",
    "    - This tells our current coroutine to pause, and immediatley schedules our ```other_coroutine``` to run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import random\n",
    "\n",
    "async def coroutine_1():\n",
    "    print('coroutine_1 is active on the event loop')\n",
    "    print('coroutine_1 yielding control. Going to be blocked for 4 seconds')\n",
    "    await asyncio.sleep(4)\n",
    "    print('coroutine_1 resumed. coroutine_1 exiting')\n",
    "    \n",
    "async def coroutine_2():\n",
    "    print('coroutine_2 is active on the event loop')\n",
    "    print('coroutine_2 yielding control. Going to be blocked for 5 seconds')\n",
    "    await asyncio.sleep(5)\n",
    "    print('coroutine_2 resumed. coroutine_2 exiting')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "coroutine_2 is active on the event loop\n",
      "coroutine_2 yielding control. Going to be blocked for 5 seconds\n",
      "coroutine_1 is active on the event loop\n",
      "coroutine_1 yielding control. Going to be blocked for 4 seconds\n",
      "coroutine_1 resumed. coroutine_1 exiting\n",
      "coroutine_2 resumed. coroutine_2 exiting\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[None, None]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create the event loop \n",
    "loop = asyncio.get_event_loop()\n",
    "\n",
    "#schedule both coroutines to run on the event loop \n",
    "loop.run_until_complete(asyncio.gather(coroutine_1(), coroutine_2()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating five tasks\n",
      "Sleeping after starting five tasks\n",
      "0 sleeps for 4.32 seconds\n",
      "1 sleeps for 3.57 seconds\n",
      "2 sleeps for 4.77 seconds\n",
      "3 sleeps for 3.04 seconds\n",
      "4 sleeps for 3.62 seconds\n",
      "Waking and waiting for five tasks\n",
      "3 awakens\n",
      "1 awakens\n",
      "4 awakens\n",
      "0 awakens\n",
      "2 awakens\n",
      "Done five tasks\n"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "import random\n",
    "\n",
    "@asyncio.coroutine\n",
    "def random_sleep(counter):\n",
    "    delay = random.random() * 5\n",
    "    print(\"{} sleeps for {:.2f} seconds\".format(counter, delay))\n",
    "    yield from asyncio.sleep(delay)\n",
    "    print(\"{} awakens\".format(counter))\n",
    "    \n",
    "@asyncio.coroutine\n",
    "def five_sleepers(): \n",
    "    print(\"Creating five tasks\") \n",
    "    tasks = [asyncio.ensure_future(random_sleep(i)) for i in range(5)]\n",
    "    print(\"Sleeping after starting five tasks\")\n",
    "    yield from asyncio.sleep(2)\n",
    "    print(\"Waking and waiting for five tasks\")\n",
    "    yield from asyncio.wait(tasks)\n",
    "    \n",
    "asyncio.get_event_loop().run_until_complete(five_sleepers())\n",
    "print(\"Done five tasks\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's piece this out: \n",
    "- ```loop.run_until_complete()``` gets the event loop and instructs it to run a future until it's done\n",
    "    - The future is ```five_sleepers()```\n",
    "- Inide of the ```five_sleepers``` future, we construct five ```random_sleep``` futures\n",
    "    - adds them to the loop's task queue so they can execute concurrently when control is returned to the event loop\n",
    "- Control is returned when we call ```yield from asyncio.sleep``` to pause execution\n",
    "    - During this break the event loop executes the tasks that are next in the queue (the five futures we just made)\n",
    "- If any sleep calls inside ```random_sleep``` are shorter than 2 seconds, the event loop passes control back into the relevant future\n",
    "    - this prints its awakening message before returning\n",
    "- When ```five_sleepers``` wakes up, it will wake up each ```random_sleep``` task "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "swepy yml",
   "language": "python",
   "name": "swepy_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
