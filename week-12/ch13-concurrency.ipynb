{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Chapter 13: Concurrency üöÄ\n",
    "\n",
    "- Threads\n",
    "- Multiprocessing\n",
    "- Futures \n",
    "- AsyncIO\n",
    "\n",
    "**By Will Norris**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Sequential Programming: \n",
    "- Sequential programming model is intuitive and natural\n",
    "    - Do things **one step at a time** (The way humans think)\n",
    "- In programming languages: \n",
    "    - Each of these real-world actions is an abstraction for a sequence of finer-grained actions\n",
    "    - Flow: ```Open the cupboard, select a tea, check water level in kettle, if low: add more, boil water, pour water in cup, wait for tea...``` \n",
    "- **But**, what we do while the water is boiling is up to us\n",
    "    - Do we just wait? \n",
    "    - Or, do we do other tasks such as starting our toast or downloading a new podcast? (*asynchronous tasks*)\n",
    "        - The whole time aware that we are waiting for our water to boil!\n",
    "- Tea kettle makers know people tend to operate asynchronously, so they add a warning for when your tea is done, to remind you to come back to the task at hand. \n",
    "    - Finding the right balance of sequentiality and asynchrony is a characteristic of efficient people, **the same is true of efficient programs**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Why Concurrency/Parallelism? \n",
    "- At some point it is not cost efficient to buy a faster machine (**scaling vertically**)\n",
    "- Instead of scaling computation up, we can go out (**scaling horizontally**)\n",
    "    - Allows us to use cheap hardware, and accomplish pieces of computation across a set of threads/processors/nodes \n",
    "- In modern computing, we can divide the problem entirely across nodes (processors) \n",
    "    - In legacy computing, we could take advantage of \"switching\", which means rapidly swapping between threads on a single process to accompish multiple things \"at once\" (time sharing systems) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Dealing with multiple things at once versus doing multiple things at once\n",
    "\n",
    "![](https://imgur.com/dFEnI85.png)\n",
    "- Concurrency: asyncio, threading in python (darn GIL ‚òπÔ∏è) \n",
    "- Parallelism: threading, multiprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Multiple Processes: \n",
    "__Motivating Factors:__\n",
    "- **Resource Utilizaton:** \n",
    "    - Programs are always waiting for external operations (File I/O), and can't do anything while they wait. Let's use that time!\n",
    "- **Fairness:**\n",
    "    - Multiple users and programs may have equal claim on the machine's resources. We want to let them share \"slices\" of time rather than give one before the other.\n",
    "- **Convenience:**\n",
    "    - It is easier to write several programs that each perform a single task and have them coordinate with each other when needed than to write one big program. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Threads: \n",
    "- Threads allow multiple streams of program control flow to coexist within a process. \n",
    "- They share process-wide resources (memory, file handles) \n",
    "    - But, each thread has its own program counter, stack, and local variables \n",
    "- Threads provide a natural decomposition for exploiting hardware parallelism when we have multiple processors\n",
    "    -  Multiple threads within the same program can be scheduled simultaneously on multi CPU's\n",
    "- Most modern OS's treat threads as **lightweight processses** and use them (not processes) as the basic unit of scheduling\n",
    "![](https://imgur.com/5mte34P.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from threading import Thread\n",
    "\n",
    "class InputReader(Thread):\n",
    "    def run(self):\n",
    "        self.line_of_text = input()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter any text and press enter: \n",
      "w\n",
      "calculated squares up to 762181 * 762181 = 580918352400\n",
      "while you typed 'w'\n"
     ]
    }
   ],
   "source": [
    "print(\"Enter any text and press enter: \")\n",
    "thread = InputReader()\n",
    "thread.start()\n",
    "\n",
    "count = result = 1\n",
    "while thread.is_alive():\n",
    "    result = count * count \n",
    "    count += 1\n",
    "\n",
    "print(\"calculated squares up to {0} * {0} = {1}\".format(\n",
    "    count, result))\n",
    "print(\"while you typed '{}'\".format(thread.line_of_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**What's going on here**\n",
    "- We have two threads, one main thread and a second created for ```InputReader```\n",
    "- While the main thread is crunching numbers\n",
    "    - The second thread pauses until it recieves our input\n",
    "    - When input is received, the thread ceases to be \"alive\" and the results are printed\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "it is 55¬∞C in Edmonton\n",
      "it is 79¬∞C in Victoria\n",
      "it is 50¬∞C in Winnipeg\n",
      "it is 48¬∞C in Fredericton\n",
      "it is 49¬∞C in Halifax\n",
      "it is 58¬∞C in Toronto\n",
      "it is 41¬∞C in Charlottetown\n",
      "it is 59¬∞C in Quebec\n",
      "it is 53¬∞C in Regina\n",
      "Got 9 temps in 0.9391319751739502 seconds\n"
     ]
    }
   ],
   "source": [
    "import json \n",
    "from urllib.request import urlopen \n",
    "import time \n",
    "import requests\n",
    "import pyowm\n",
    "\n",
    "CITIES = ['Edmonton', 'Victoria', 'Winnipeg', 'Fredericton',\n",
    "          'Halifax', 'Toronto', 'Charlottetown',\n",
    "          'Quebec', 'Regina']\n",
    "\n",
    "class TempGetter(Thread):\n",
    "    def __init__(self, city):\n",
    "        super().__init__()\n",
    "        self.city = city\n",
    "        self.owm = pyowm.OWM('be06b12aa45e1ca05a8f972f81376c6d')\n",
    "    def run(self):\n",
    "        city = self.owm.weather_at_place(self.city)\n",
    "        weather = city.get_weather()\n",
    "        self.temperature = weather.get_temperature('fahrenheit')['temp']\n",
    "        \n",
    "threads = [TempGetter(c) for c in CITIES]\n",
    "start = time.time()\n",
    "for thread in threads:\n",
    "    thread.start()\n",
    "for thread in threads:\n",
    "    thread.join()\n",
    "for thread in threads:\n",
    "    print(\"it is {0.temperature:.0f}¬∞C in {0.city}\".format(thread))\n",
    "print(\n",
    "   \"Got {} temps in {} seconds\".format(\n",
    "   len(threads), time.time() - start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### What's actually happening here? \n",
    "- 10 threads are started\n",
    "    - Remember to call ```super``` to ensure we instantiate an actual ```Thread``` object. \n",
    "    - We construct 10 thread objects from within the main thread, then run them later. \n",
    "        - Data constructed in one thread is accessible from other running threads\n",
    "- Each thread is joined with eachother \n",
    "    - Joining threads tells each one to \"wait for the thread to complete before doing anything\" \n",
    "    - This means the second for loop won't end until all 10 threads have finished \n",
    "- **In threads, all state is shared by default**\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Running on one thread instead, much much slower!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "it is 55.27¬∞C in Edmonton\n",
      "it is 78.85¬∞C in Victoria\n",
      "it is 50.0¬∞C in Winnipeg\n",
      "it is 48.31¬∞C in Fredericton\n",
      "it is 48.94¬∞C in Halifax\n",
      "it is 57.87¬∞C in Toronto\n",
      "it is 41.18¬∞C in Charlottetown\n",
      "it is 58.78¬∞C in Quebec\n",
      "it is 53.37¬∞C in Regina\n",
      "Got 9 temps in 2.9616730213165283 seconds\n"
     ]
    }
   ],
   "source": [
    "owm = pyowm.OWM('be06b12aa45e1ca05a8f972f81376c6d')\n",
    "start = time.time()\n",
    "for city in CITIES:\n",
    "    cityobj = owm.weather_at_place(city)\n",
    "    weather = cityobj.get_weather()\n",
    "    temperature = weather.get_temperature('fahrenheit')['temp']\n",
    "    print(\"it is {0}¬∞C in {1}\".format(temperature, city))\n",
    "print(\n",
    "   \"Got {} temps in {} seconds\".format(\n",
    "   len(threads), time.time() - start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Threads sound great! What's the catch?? \n",
    "- Python programmers avoid threading for several reasons: \n",
    "    - Better alternative methods to concurrent programming in Python\n",
    "    - Shared Memory\n",
    "    - **Global Interpreter Lock** - (The GIL)\n",
    "    - Thread Overhead"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Shared Memory:**\n",
    "- Shared memory is both a major advantage and disadvantage of threading\n",
    "    - It is convenient to have access to all variables in memory from any thread \n",
    "    - However, this can cause horrible inconsistencies in the program state \n",
    "        - It is easy to allow one thread to change a value that another thread expected, which can cause unknown errors. \n",
    "- We can \"synchronize\" thread's access to variables, however this can get complex and improper synchronization can be hard to track down."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**The Global Interpreter Lock:**\n",
    "- To efficiently manage memory , garbage collection, and calls to machine code in libraries, Python uses the Global Interpreter Lock (GIL)\n",
    "- It is impossible to turn off and it makes it impossible to properly use threads for parallel processing in python\n",
    "    - The GIL will prevent any two thread's from doing work at the exact same time, even if they have work to do. (\"doing work\" == using CPU) \n",
    "    - The GIL is released as soon as the thread starts to wait for anything \n",
    "    \n",
    "**Why do we still have the GIL?**\n",
    "- It makes the reference implementation much easier to maintain (language structure)\n",
    "- It makes single core python faster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Thread Overhead:**\n",
    "- Each thread takes up some memory (both in python and the OS kernel) to keep track of the thread state \n",
    "- Switching (jumping between threads) uses some CPU time\n",
    "    - This can be improved with proper thread management and the use of ```ThreadPool``` to help reuse threads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Multiprocessing\n",
    "- Multiprocessing is designed for when CPU-intensive jobs can be run in parallel and mulitple cores are available. \n",
    "    - There are almost always multiple cores available now! (4-core Rasberry pi = $30)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My computer has 4 available processes/cpu's to run on.\n"
     ]
    }
   ],
   "source": [
    "from multiprocessing import Process, cpu_count\n",
    "\n",
    "print(\"My computer has {} available processes/cpu's to run on.\".format(cpu_count()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4072\n",
      "4073\n",
      "4071\n",
      "4074\n",
      "work took 8.521000146865845 seconds\n"
     ]
    }
   ],
   "source": [
    "import time \n",
    "import os \n",
    "\n",
    "class MuchCPU(Process):\n",
    "    def run(self):\n",
    "        print(os.getpid())\n",
    "        for i in range(200000000):\n",
    "            pass\n",
    "        \n",
    "procs = [MuchCPU() for f in range(cpu_count())]\n",
    "t = time.time()\n",
    "for p in procs:\n",
    "    p.start()\n",
    "for p in procs: \n",
    "    p.join()\n",
    "print(\"work took {} seconds\".format(time.time()-t))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- I have a 2-core (4 virtual cores) machine, so python will spin up 4 different processes each with a unique pid (process ID) \n",
    "- This took me 8.28 seconds versus 12.96 seconds on a 2014 similar setup. \n",
    "    - This is a 36% increase in performance without increasing core count\n",
    "    \n",
    "- When I run this process on my laptop, we can see that all 4 cores hit 100% usage for a brief moment.\n",
    "    - This is because each core is simultaneously crunching 2,000,000 numbers\n",
    "    \n",
    "![](https://imgur.com/zzR5qxm.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Here we subclass on ```Thread``` instead of ```Process```\n",
    "- This means that each thread is sharing the same process (only using 1 process)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2001\n",
      "2001\n",
      "20012001\n",
      "\n",
      "work took 17.586171865463257 seconds\n"
     ]
    }
   ],
   "source": [
    "class MuchCPU(Thread):\n",
    "    def run(self):\n",
    "        print(os.getpid())\n",
    "        for i in range(200000000):\n",
    "            pass\n",
    "        \n",
    "procs = [MuchCPU() for f in range(cpu_count())]\n",
    "t = time.time()\n",
    "for p in procs:\n",
    "    p.start()\n",
    "for p in procs: \n",
    "    p.join()\n",
    "print(\"work took {} seconds\".format(time.time()-t))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Why is it not 4x slower?**\n",
    "- When we use all 4 processes on my laptop, they have to share computation with the other stuff happening on my laptop. \n",
    "- When we use a single process and multithread, the remaining three processes can do laptop stuff "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Multiprocessing Pools\n",
    "- It can get really hard to allocate processes for each task. \n",
    "- We need something to manage where/when code is running and when each process interacts with eachother.\n",
    "- Pools are designed to distribute tasks to available processors! \n",
    "    - Use FIFO scheduling \n",
    "    - Map reduce architecture: \n",
    "        - Maps input to different processors and collects the output from all the processors\n",
    "        - After execution, returns output in form of a list or array\n",
    "        - Nothing is returned until all processes finish"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### When Should I use Pool or Process???\n",
    "**More Complex Systems Use More Resources, Basically Always**\n",
    "\n",
    "**Pool**\n",
    "- Pool allows you to do multiple jobs per process, which can make your program easier to parallelize\n",
    "- Much better for large jobs\n",
    "- Example: You have a million tasks to execute in parallel\n",
    "    - You can create a ```Pool``` with ```num_processes = CPU_count``` and pass them to ```pool.map```\n",
    "    - The pool will distribute those tasks to the worker processes and collects the return values in a list for the parent process\n",
    "    \n",
    "**Process**\n",
    "- If you have a small subset of tasks then a Pool may be overkill for the task \n",
    "    - All the pickling and moving and scheduling slows things down!\n",
    "    - There is overhead to having such a clean and organized system!!\n",
    "- File IO\n",
    "    - In pooling if there is a long IO operation, due to the FIFO scheduling, the core will wait to schedule another process\n",
    "    - Process would suspend the IO process and schedule another one "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Processs a\tWaiting 2 seconds\n",
      " Processs b\tWaiting 4 seconds\n",
      " Process a\tDONE\n",
      " Processs c\tWaiting 6 seconds\n",
      " Process b\tDONE\n",
      " Processs d\tWaiting 8 seconds\n",
      " Process c\tDONE\n",
      " Processs e\tWaiting 1 seconds\n",
      " Process e\tDONE\n",
      " Processs f\tWaiting 3 seconds\n",
      " Process d\tDONE\n",
      " Processs g\tWaiting 5 seconds\n",
      " Process f\tDONE\n",
      " Processs h\tWaiting 7 seconds\n",
      " Process g\tDONE\n",
      " Process h\tDONE\n"
     ]
    }
   ],
   "source": [
    "import random \n",
    "import time\n",
    "import multiprocessing\n",
    "\n",
    "data = (\n",
    "    ['a', '2'], ['b', '4'], ['c', '6'], ['d', '8'],\n",
    "    ['e', '1'], ['f', '3'], ['g', '5'], ['h', '7']\n",
    ")\n",
    "\n",
    "def mp_worker(input1):\n",
    "    inputs = input1[0]\n",
    "    the_time = input1[1]\n",
    "    print(\" Processs {}\\tWaiting {} seconds\".format(inputs, the_time))\n",
    "    time.sleep(int(the_time))\n",
    "    print(\" Process {}\\tDONE\".format(inputs))\n",
    "\n",
    "def mp_handler():\n",
    "    p = multiprocessing.Pool(2)\n",
    "    p.map(mp_worker, data)\n",
    "\n",
    "mp_handler()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### So, what is happening here? \n",
    "- We have two pools to corrall our processes into. This means that each pool will automatically take the next task in our list of tasks as they finish. \n",
    "- We can see that the next task in the list is automatically assigned to whichever pool frees up first"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Queues: \n",
    "- If we need to better control how information is passed between processes, then we can use queues\n",
    "- Queues are good for sending messages from one process into one or more other processes\n",
    "    - Any picklable object can be sent into a Queue\n",
    "        - Pickling is expensive, especially on large objects!!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Queues Example:**\n",
    "- We have a very simple reader and writer of messages off of a queue \n",
    "- The writer sends a lot of integers to the reader \n",
    "    - when the writer runs out of numbers, it sends a ```\"DONE\"``` message\n",
    "    - when the reader receives a ```\"DONE\"``` message, it breaks the read loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sending 10000 numbers to Queue() took 0.24085092544555664 seconds\n",
      "Sending 100000 numbers to Queue() took 2.5450048446655273 seconds\n",
      "Sending 1000000 numbers to Queue() took 25.990029096603394 seconds\n"
     ]
    }
   ],
   "source": [
    "from multiprocessing import Process, Queue\n",
    "import time\n",
    "import sys \n",
    "\n",
    "def reader_proc(queue):\n",
    "    # Read from the queue. spawned as seperate process \n",
    "    while True:\n",
    "        msg = queue.get() # just read from queue \n",
    "        if (msg == \"DONE\"):\n",
    "            break\n",
    "            \n",
    "def writer(count, queue):\n",
    "    # Write to our queue\n",
    "    for ii in range(0, count):\n",
    "        queue.put(ii)\n",
    "    queue.put('DONE')\n",
    "        \n",
    "pqueue = Queue()\n",
    "for count in [10**4, 10**5, 10**6]:\n",
    "    # reader_proc() reads pqueue as sperate process\n",
    "    reader_p = Process(target=reader_proc, args=((pqueue),))\n",
    "    reader_p.daemon = True\n",
    "    reader_p.start()\n",
    "    \n",
    "    _start = time.time()\n",
    "    writer(count, pqueue) # writer adds numbers to queue\n",
    "    reader_p.join() # join is what starts the communication\n",
    "    print(\"Sending {0} numbers to Queue() took {1} seconds\".format(count,(time.time()-_start)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### What is happening here? \n",
    "- We have two functions, a reader and a writer, which are spawned as seperate processes \n",
    "- We instantiate a seperate process for the reader with the queue as its argument\n",
    "- When ```join()``` is called, we join our seperate process to our main process, where the writer has been adding numbers to the queue "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## So... What's the Catch with Multiprocessing? \n",
    "\n",
    "- **\"There is no best way to do concurrency; this is especially true in Python\"**\n",
    "    - Every problem is approached differently in parallel, part of the challenge is choosing the best way to parallelize. \n",
    "\n",
    "- Primary drawback with multiprocessing is that sharing data between processes is super expensive. \n",
    "    - We have to employ some data structure that pickles data (queue, pipe, etc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Due to the overhead of communication, **multicprocessing is best applied when:**\n",
    "- Information passed between processes is small \n",
    "- The work done in each individual process is very large\n",
    "\n",
    "Additional Drawbacks:\n",
    "- If we don't need to communicate at all between processes, then we might as well just write seperate independent programs (**niche use case**) \n",
    "- It can also be very difficult to tell which process is accessing each variable in memory and when\n",
    "    - It is easily possible for one process to overwrite a variable in memory when another process expects it to remain unchanged "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Asynchronous Processing: \n",
    "- Not direcly related to threading or mulitprocessing. \n",
    "- Async processing focuses on never wasting time waiting for a task to complete on a given process\n",
    "    - If we need to wait for a databases queary, let's let the next task do something while we wait!\n",
    "    - Many times, processing that we are waiting for occurs on a seperate machine (database queary, POST request, etc.)\n",
    "        - This means our machine can use its time better"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Futures\n",
    "- Futures \"Provide a high-level interface for asynchronously executing callables\" - Python 3.x docs\n",
    "- They allow us to structure our code such that it is easier to track down when we alter the shared state\n",
    "- A future is just an object that wraps a function call. \n",
    "    - The function is run in the background in a thread or process \n",
    "    - Checks if things have completed and get results when they do \n",
    "    \n",
    "In Computer Science, *future* refers to a construct that can be used for synchronization when using concurrent programming techniques\n",
    "- The future is a way to describe the result of a process or thread before it has finished processing (pending result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from concurrent.futures import as_completed\n",
    "import urllib.request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "def downloader(url):\n",
    "    req = urllib.request.urlopen(url)\n",
    "    filename = os.path.basename(url)\n",
    "    ext = os.path.splitext(url)[1]\n",
    "    if not ext: \n",
    "        raise RuntimeError(\"URL does not contain an extension\")\n",
    "    \n",
    "    with open(filename, 'wb') as file_handle:\n",
    "        while True:\n",
    "            chunk = req.read(1024)\n",
    "            if not chunk:\n",
    "                break\n",
    "            file_handle.write(chunk)\n",
    "    msg = 'Finished downloading {filename}'.format(filename=filename)\n",
    "    return msg\n",
    "\n",
    "def main(urls):\n",
    "    with ThreadPoolExecutor(max_workers=5) as executor:\n",
    "        futures = [executor.submit(downloader, url) for url in urls]\n",
    "        for future in as_completed(futures):\n",
    "            print(future.result())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished downloading f1040sb.pdf\n",
      "Finished downloading f1040.pdf\n",
      "Finished downloading f1040es.pdf\n",
      "Finished downloading f1040a.pdf\n",
      "Finished downloading f1040ez.pdf\n"
     ]
    }
   ],
   "source": [
    "urls = [\"http://www.irs.gov/pub/irs-pdf/f1040.pdf\",\n",
    "            \"http://www.irs.gov/pub/irs-pdf/f1040a.pdf\",\n",
    "            \"http://www.irs.gov/pub/irs-pdf/f1040ez.pdf\",\n",
    "            \"http://www.irs.gov/pub/irs-pdf/f1040es.pdf\",\n",
    "            \"http://www.irs.gov/pub/irs-pdf/f1040sb.pdf\"]\n",
    "main(urls)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### So, what's actually going on here? \n",
    "- In main we instantiate our thread pool: \n",
    "    - Our pool has 5 workers\n",
    "    - We use list comprehension to create a group of futures (jobs) and then we call the ```as_complete()```.\n",
    "        - This function is an iterator that yields the futures as they complete. \n",
    "        - When they complete, we print the result, which is the string returned from ```downloader()```\n",
    "\n",
    "**Futures' Structure:** \n",
    "- Once the ```executor``` has been constructed, we can ```submit``` jobs to it\n",
    "    - The ```submit()``` method immediately returns a ```Future``` object, which is a promise to give a result eventually\n",
    "- Futures have built in ```queue``` functionality: \n",
    "    - We can construct a ```queue``` of futures (essentially all running) and loop through the queue over and over\n",
    "        - If the future at the end of the queue is still running, put it back at the front of the queue\n",
    "        - If the future is still running, then get the ```result()```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## AsyncIO: \n",
    "**Current state of the art in Python concurrent programming**\n",
    "- Combines futures, coroutines, and an Event Loop\n",
    "- Can be used for a few different concurrent tasks, but designed for network I/O\n",
    "    - Most network apps spend a lot of time waiting for data to come in\n",
    "    - We can handle each client in a seperate thread, but threads use memory and other resources, so AsyncIO uses coroutines\n",
    "\n",
    "**Example:** You have a single core machine. You receive a request and need to make two database queries. Each query takes 50ms. \n",
    "- Synchronously: You must completely resolve the first query before starting the second. Total time = 100ms\n",
    "- Asynchronous: You can send off query one, send off query two, then wait for each one individually. Total time = 50ms\n",
    "    \n",
    "**From Hackernoon.com:**\n",
    "\n",
    "    \"Asyncio is a beautiful symphony between an Event loop, Tasks and Coroutines all coming together so perfectly‚Ää‚Äî‚Ääits going to make you cry.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![](https://imgur.com/yhoCa8X.png)\n",
    "-- Source Chlyd Medford (Real Python)\n",
    "\n",
    "- Once a process leaves the event loop, it is running externally\n",
    "    - This means we could potentially have our \"Intensive Operations\" running in parallel if we want.\n",
    "    - Often times these operations are outside of python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Event Loop\n",
    "- In its most simple form, and Event Loop is just a loop that runs tasks one at a time.\n",
    "- The special part is that when the running task makes a blocking call (network request)\n",
    "    - The event loop can let another task take a turn, and remembers roughly how long the blocking task will be waiting \n",
    "    \n",
    "    *\"The event loop time is precious. If you are not making progress, you should step off the loop, so that someone else can. Event loop is the measure of progress\" -- Miklos Philips*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### The Coroutine & Task\n",
    "- Coroutines are essentially *stateful* functions (very similar to generators in python)\n",
    "    - When a coroutine is waiting for something it can give up control of the event loop, but save it's state for when it is ready again\n",
    "- To pause a couroutine we use ```await other_coroutine```\n",
    "    - This tells our current coroutine to pause, and immediatley schedules our ```other_coroutine``` to run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### What is a coroutine exactly?\n",
    "- Coroutines give you many of the advantages of threads without the huge disadvantages of threads (difficult to coordinate, memory hogs, costly to start up) \n",
    "- Coroutines let you have many *seemingly* simultaneous functions in a python program \n",
    "    - Implemented as an extension to generators \n",
    "    - The cost of starting a coroutine is equal to a function call and they use < 1KB of memory (Thread uses > 8KB)\n",
    "- Coroutines enable the code consuming a generator to ```send``` a value back into the generator function after each ```yield```\n",
    "    - The generator function receives the value passed to the ```send``` function as the result of the corresonding ```yield``` expression\n",
    "    \n",
    "**The true magic of coroutines is that they are independent like threads, but they pause at each ```yield``` and resume after each call to ```send```**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "4\n",
      "4\n",
      "-1\n"
     ]
    }
   ],
   "source": [
    "def minimize():\n",
    "    current = yield\n",
    "    while True:\n",
    "        value = yield current\n",
    "        current = min(value, current)\n",
    "\n",
    "it = minimize()\n",
    "next(it)            # Prime the generator\n",
    "print(it.send(10))\n",
    "print(it.send(4))\n",
    "print(it.send(22))\n",
    "print(it.send(-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import random\n",
    "\n",
    "async def coroutine_1():\n",
    "    print('coroutine_1 is active on the event loop')\n",
    "    print('coroutine_1 yielding control. Going to be blocked for 4 seconds')\n",
    "    await asyncio.sleep(4)\n",
    "    print('coroutine_1 resumed. coroutine_1 exiting')\n",
    "    \n",
    "async def coroutine_2():\n",
    "    print('coroutine_2 is active on the event loop')\n",
    "    print('coroutine_2 yielding control. Going to be blocked for 5 seconds')\n",
    "    await asyncio.sleep(5)\n",
    "    print('coroutine_2 resumed. coroutine_2 exiting')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "coroutine_2 is active on the event loop\n",
      "coroutine_2 yielding control. Going to be blocked for 5 seconds\n",
      "coroutine_1 is active on the event loop\n",
      "coroutine_1 yielding control. Going to be blocked for 4 seconds\n",
      "coroutine_1 resumed. coroutine_1 exiting\n",
      "coroutine_2 resumed. coroutine_2 exiting\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[None, None]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create the event loop \n",
    "loop = asyncio.get_event_loop()\n",
    "\n",
    "#schedule both coroutines to run on the event loop \n",
    "loop.run_until_complete(asyncio.gather(coroutine_1(), coroutine_2()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating five tasks\n",
      "Sleeping after starting five tasks\n",
      "0 sleeps for 3.64 seconds\n",
      "1 sleeps for 1.42 seconds\n",
      "2 sleeps for 3.69 seconds\n",
      "3 sleeps for 3.21 seconds\n",
      "4 sleeps for 4.68 seconds\n",
      "1 awakens\n",
      "Waking and waiting for five tasks\n",
      "3 awakens\n",
      "0 awakens\n",
      "2 awakens\n",
      "4 awakens\n",
      "Done five tasks\n"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "import random\n",
    "\n",
    "@asyncio.coroutine\n",
    "def random_sleep(counter):\n",
    "    delay = random.random() * 5\n",
    "    print(\"{} sleeps for {:.2f} seconds\".format(counter, delay))\n",
    "    yield from asyncio.sleep(delay)\n",
    "    print(\"{} awakens\".format(counter))\n",
    "    \n",
    "@asyncio.coroutine\n",
    "def five_sleepers(): \n",
    "    print(\"Creating five tasks\") \n",
    "    tasks = [asyncio.ensure_future(random_sleep(i)) for i in range(5)]\n",
    "    print(\"Sleeping after starting five tasks\")\n",
    "    yield from asyncio.sleep(2)\n",
    "    print(\"Waking and waiting for five tasks\")\n",
    "    yield from asyncio.wait(tasks)\n",
    "    \n",
    "asyncio.get_event_loop().run_until_complete(five_sleepers())\n",
    "print(\"Done five tasks\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Let's piece this out: \n",
    "- ```loop.run_until_complete()``` gets the event loop and instructs it to run a future until it's done\n",
    "    - The future is ```five_sleepers()```\n",
    "- Inide of the ```five_sleepers``` future, we construct five ```random_sleep``` futures\n",
    "    - adds them to the loop's task queue so they can execute concurrently when control is returned to the event loop\n",
    "- Control is returned when we call ```yield from asyncio.sleep``` to pause execution\n",
    "    - During this break the event loop executes the tasks that are next in the queue (the five futures we just made)\n",
    "- If any sleep calls inside ```random_sleep``` are shorter than 2 seconds, the event loop passes control back into the relevant future\n",
    "    - this prints its awakening message before returning\n",
    "- When ```five_sleepers``` wakes up, it will wake up each ```random_sleep``` task in the order they should have woken up!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## What makes AsyncIO So Special? \n",
    "- Since the event loop manages the order of execution of all tasks in a very controlled manner, we can write code that executes synchonously until we actually need to wait for something\n",
    "    - This means we don't have to worry about the nondeterministic behavior of threads, which means the shared state is much less of a problem! \n",
    "        - Basically, switching is fully managed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Async Strengths and Weaknesses\n",
    "\n",
    "**Async Strengths**\n",
    "- Any time we have multiple IO-bound tasks\n",
    "    - Network IO (client or server side) \n",
    "    - Serverless designs: peer to peer, multi-user network (group chatroom)\n",
    "    \n",
    "**Async Weaknesses**\n",
    "- ```await``` only supports a limited set of object types \n",
    "    - If we have DBMS objects that arent supported, we will have to build a wrapper (lots of work!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## So Many Choices, Which One Do I Use When?\n",
    "**AsyncIO vs Multiprocessing**\n",
    "- If you have multiple, fairly uniform CPU-bound tasks (scikit-learn, keras grid searches)\n",
    "    - Multiprocessing no doubt!\n",
    "    - Async is good at letting one process share the load, but if we need to do a lot of CPU work, we just need more processes\n",
    "- Many, many blocking calls can slow an async system down\n",
    "- Ultimately there is a thin line of when multiprocessing or asyncIO would be better for a given task\n",
    "\n",
    "**AsyncIO vs Threading**\n",
    "- Threading in Python doesn't work like we want, and async works similarly but better\n",
    "- Threading is hard! \n",
    "    - It is infamous for being difficult for tracing bugs  (race conditions, memory usage issues)\n",
    "- AsyncIO scales better than threading\n",
    "    - Threads are a system resource with finite availability \n",
    "        - So we can't just make thousands of them!\n",
    "    - Async tasks (futures) can be created in massive quantities "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Additional Examples: \n",
    "1. Very simple single coroutine\n",
    "2. Multiple coroutine example \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simple Coroutine Example\n"
     ]
    }
   ],
   "source": [
    "async def myCoroutine():\n",
    "    print(\"Simple Coroutine Example\")\n",
    "    \n",
    "def main():\n",
    "    \"\"\"\n",
    "    main function to define event loop and run loop until done\n",
    "    \"\"\"\n",
    "    loop = asyncio.get_event_loop()\n",
    "    loop.run_until_complete(myCoroutine())\n",
    "    loop.close()\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# alternate method for defining coroutines (not as common)\n",
    "@asyncio.coroutine\n",
    "def myCoroutine2():\n",
    "    print(\"My Coroutine\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Multiple Coroutines: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "async def myCoroutine(id):\n",
    "    process_time = random.randint(1,5)\n",
    "    await asyncio.sleep(process_time)\n",
    "    print(\"Coroutine {}, has succesfully completed after {} seconds\".format(id, process_time))\n",
    "    \n",
    "    \n",
    "async def main():\n",
    "    \"\"\"\n",
    "    main function to define event loop and run loop until done\n",
    "    \"\"\"\n",
    "    tasks = []\n",
    "    for i in range(10):\n",
    "        tasks.append(asyncio.ensure_future(myCoroutine(i)))\n",
    "    await asyncio.gather(*tasks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coroutine 1, has succesfully completed after 1 seconds\n",
      "Coroutine 5, has succesfully completed after 2 seconds\n",
      "Coroutine 7, has succesfully completed after 3 seconds\n",
      "Coroutine 0, has succesfully completed after 4 seconds\n",
      "Coroutine 2, has succesfully completed after 4 seconds\n",
      "Coroutine 4, has succesfully completed after 4 seconds\n",
      "Coroutine 9, has succesfully completed after 4 seconds\n",
      "Coroutine 3, has succesfully completed after 5 seconds\n",
      "Coroutine 6, has succesfully completed after 5 seconds\n",
      "Coroutine 8, has succesfully completed after 5 seconds\n",
      "Total time: 5.005584001541138 seconds.\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "loop = asyncio.new_event_loop()\n",
    "loop.run_until_complete(main())\n",
    "loop.close()\n",
    "print(\"Total time: {} seconds.\".format(time.time()-start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### If we do the same process without Asyncio, it is much much slower"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Process 0 finished in 3 seconds.\n",
      "Process 1 finished in 2 seconds.\n",
      "Process 2 finished in 3 seconds.\n",
      "Process 3 finished in 4 seconds.\n",
      "Process 4 finished in 2 seconds.\n",
      "Process 5 finished in 2 seconds.\n",
      "Process 6 finished in 2 seconds.\n",
      "Process 7 finished in 1 seconds.\n",
      "Process 8 finished in 5 seconds.\n",
      "Process 9 finished in 4 seconds.\n",
      "Total time: 28.03645420074463 seconds.\n"
     ]
    }
   ],
   "source": [
    "def run_without_asyncio():\n",
    "    for i in range(10):\n",
    "        process_time = random.randint(1,5)\n",
    "        time.sleep(process_time)\n",
    "        print(\"Process {} finished in {} seconds.\".format(i, process_time))\n",
    "start = time.time()      \n",
    "run_without_asyncio()\n",
    "print(\"Total time: {} seconds.\".format(time.time()-start))"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "swepy yml",
   "language": "python",
   "name": "swepy_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
