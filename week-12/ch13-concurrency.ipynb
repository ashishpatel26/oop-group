{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Chapter 13: Concurrency ðŸš€\n",
    "\n",
    "- Threads\n",
    "- Multiprocessing\n",
    "- Futures \n",
    "- AsyncIO\n",
    "\n",
    "**By Will Norris**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Sequential Programming: \n",
    "- Sequential programming model is intuitive and natural\n",
    "    - Do things **one step at a time** (The way humans think)\n",
    "- In programming languages: \n",
    "    - Each of these real-world actions is an abstraction for a sequence of finer-grained actions\n",
    "    - Flow: ```Open the cupboard, select a tea, check water level in kettle, if low: add more, boil water, pour water in cup, wait for tea...``` \n",
    "- **But**, what we do while the water is boiling is up to us\n",
    "    - Do we simply wait? \n",
    "    - Or, do we do other tasks such as starting our toast or fetching the newspaper (asynchronous tasks)\n",
    "        - The whole time aware that we are waiting for our water to boil!\n",
    "- Tea kettle makers know people tend to operate asynchronously, so they add a warning for when your tea is done, to remind you to come back to the task at hand. \n",
    "    - Finding the right balance of sequentiality and asynchrony is a characteristic of efficient people, **the same is true of efficient programs**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Why Concurrency? \n",
    "- At some point it is not cost efficient to buy a faster machine (**scaling vertically**)\n",
    "- Instead of scaling computation up, we can go out (**scaling horizontally**)\n",
    "    - Allows us to use cheap hardware, and accomplish pieces of computation across a set of threads/processors/nodes \n",
    "- In modern computing, we can divide the problem entirely across nodes (processors) \n",
    "    - In legacy computing, we could take advantage of \"switching\", which means rapidly swapping between threads on a single process to accompish multiple things \"at once\" (time sharing systems) \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Multiple Processes: \n",
    "__Motivating Factors:__\n",
    "- **Resource Utilizaton:** \n",
    "    - Programs are always waiting for external operations (File I/O), and can't do anything while they wait. Let's use that time!\n",
    "- **Fairness:**\n",
    "    - Multiple users and programs may have equal claim on the machine's resources. We want to let them share \"slices\" of time rather than give one before the other \n",
    "- **Convenience:**\n",
    "    - It is easier to write several programs that each perform a single task and have them coordinate with each other when needed than to write one big program. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Threads: \n",
    "- Threads allow multiple streams of program control flow to coexist within a process. \n",
    "- They share process-wide resources (memory, file handles) \n",
    "    - But, each thread has its own program counter, stack, and local variables \n",
    "- Threads provide a natural decomposition for exploiting hardware parallelism when we have multiple processors\n",
    "    -  multiple threads within the same program can be scheduled simultaneously on multi CPU's\n",
    "- Most modern OS's treat threads as **lightweight processses** and use them (not processes) as the basic unit of scheduling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![](https://imgur.com/5mte34P.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from threading import Thread\n",
    "\n",
    "class InputReader(Thread):\n",
    "    def run(self):\n",
    "        self.line_of_text = input()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter any text and press enter: \n",
      "wi\n",
      "calculated squares up to 1712925 * 1712925 = 2934108629776\n",
      "while you typed 'wi'\n"
     ]
    }
   ],
   "source": [
    "print(\"Enter any text and press enter: \")\n",
    "thread = InputReader()\n",
    "thread.start()\n",
    "\n",
    "count = result = 1\n",
    "while thread.is_alive():\n",
    "    result = count * count \n",
    "    count += 1\n",
    "\n",
    "print(\"calculated squares up to {0} * {0} = {1}\".format(\n",
    "    count, result))\n",
    "print(\"while you typed '{}'\".format(thread.line_of_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "it is 40Â°C in Edmonton\n",
      "it is 79Â°C in Victoria\n",
      "it is 31Â°C in Winnipeg\n",
      "it is 54Â°C in Fredericton\n",
      "it is 54Â°C in Halifax\n",
      "it is 56Â°C in Toronto\n",
      "it is 44Â°C in Charlottetown\n",
      "it is 44Â°C in Quebec\n",
      "it is 39Â°C in Regina\n",
      "Got 9 temps in 0.19599175453186035 seconds\n"
     ]
    }
   ],
   "source": [
    "import json \n",
    "from urllib.request import urlopen \n",
    "import time \n",
    "import requests\n",
    "import pyowm\n",
    "\n",
    "CITIES = ['Edmonton', 'Victoria', 'Winnipeg', 'Fredericton',\n",
    "          'Halifax', 'Toronto', 'Charlottetown',\n",
    "          'Quebec', 'Regina']\n",
    "\n",
    "class TempGetter(Thread):\n",
    "    def __init__(self, city):\n",
    "        super().__init__()\n",
    "        self.city = city\n",
    "        self.owm = pyowm.OWM('be06b12aa45e1ca05a8f972f81376c6d')\n",
    "    def run(self):\n",
    "        city = self.owm.weather_at_place(self.city)\n",
    "        weather = city.get_weather()\n",
    "        self.temperature = weather.get_temperature('fahrenheit')['temp']\n",
    "        \n",
    "threads = [TempGetter(c) for c in CITIES]\n",
    "start = time.time()\n",
    "for thread in threads:\n",
    "    thread.start()\n",
    "for thread in threads:\n",
    "    thread.join()\n",
    "for thread in threads:\n",
    "    print(\"it is {0.temperature:.0f}Â°C in {0.city}\".format(thread))\n",
    "print(\n",
    "   \"Got {} temps in {} seconds\".format(\n",
    "   len(threads), time.time() - start))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### What's actually happening here? \n",
    "- 10 threads are started\n",
    "    - Remember to call ```super``` to ensure we instantiate an actual ```Thread``` object. \n",
    "    - We construct 10 thread objects from within the main thread, then run them later. \n",
    "        - Data constructed in one thread is accessible from other running threads\n",
    "- Each thread is joined with eachother \n",
    "    - Joining threads tells each one to \"wait for the thread to complete before doing anything\" \n",
    "    - This means the second for loop won't end until all 10 threads have finished \n",
    "- **In threads, all state is shared by default**\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Running on one thread instead, much slower:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "it is 40Â°C in Edmonton\n",
      "it is 79Â°C in Victoria\n",
      "it is 32Â°C in Winnipeg\n",
      "it is 54Â°C in Fredericton\n",
      "it is 54Â°C in Halifax\n",
      "it is 58Â°C in Toronto\n",
      "it is 43Â°C in Charlottetown\n",
      "it is 43Â°C in Quebec\n",
      "it is 40Â°C in Regina\n",
      "Got 9 temps in 1.6550121307373047 seconds\n"
     ]
    }
   ],
   "source": [
    "threads = [TempGetter(c) for c in CITIES]\n",
    "start = time.time()\n",
    "for thread in threads:\n",
    "    thread.run()\n",
    "for thread in threads:\n",
    "    print(\"it is {0.temperature:.0f}Â°C in {0.city}\".format(thread))\n",
    "print(\n",
    "   \"Got {} temps in {} seconds\".format(\n",
    "   len(threads), time.time() - start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Threads sound great! What's the catch?? \n",
    "- Python programmers avoid threading for several reasons: \n",
    "    - Better alternative methods to concurrent programming in Python\n",
    "    - Shared Memory\n",
    "    - Global Interpreter Lock\n",
    "    - Thread Overhead"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Shared Memory:**\n",
    "- Shared memory is both a major advantage and disadvantage of threading\n",
    "    - It is convenient to have access to all variables in memory from any thread \n",
    "    - However, this can cause horrible inconsistencies in the program state \n",
    "        - It is easy to allow one thread to change a value that another thread expected, which can cause unknown errors. \n",
    "- We can \"synchronize\" thread's access to variables, however this can get complex and improper synchronization can be hard to find."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**The Global Interpreter Lock:**\n",
    "- To efficiently manage memory , garbage collection, and calls to machine code in libraries, Python uses the Global Interpreter Lock (GIL)\n",
    "- It is impossible to turn off and it makes it impossible to properly use threads for parralell processing in python\n",
    "    - The GIL will prevent any two thread's from doing work at the exact same time, even if they have work to do. (\"doing work\" == using CPU) \n",
    "    - The GIL is released as soon as the thread starts to wait for anything \n",
    "    \n",
    "**Why do we still have the GIL?**\n",
    "- It makes the reference implementation much easier to maintain (language structure)\n",
    "- It makes single core python faster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Thread Overhead:**\n",
    "- Each thread takes up some memory (both in python and the OS kernel) to keep track of the thread state \n",
    "- Switching (jumping between threads) uses some CPU time\n",
    "    - This can be improved with "
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "swepy yml",
   "language": "python",
   "name": "swepy_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
